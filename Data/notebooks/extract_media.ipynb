{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193cf601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reddit packages loaded successfully\n",
      "üìä Legislative Bill Mentions Data Collector\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Legislative Bill Mentions Data Collection\n",
    "# Erdos Institute Project: Predicting Congressional Bill Outcomes\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install required packages first\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages if not available\"\"\"\n",
    "    packages = ['praw', 'pmaw', 'requests', 'pandas', 'numpy', 'matplotlib', 'seaborn']\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}...\")\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "\n",
    "# Install packages\n",
    "install_packages()\n",
    "\n",
    "# Import packages\n",
    "try:\n",
    "    import praw\n",
    "    from pmaw import PushshiftAPI\n",
    "    REDDIT_AVAILABLE = True\n",
    "    print(\"‚úÖ Reddit packages loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Reddit packages not available: {e}\")\n",
    "    print(\"Note: You can still use Guardian and Common Crawl collectors\")\n",
    "    REDDIT_AVAILABLE = False\n",
    "\n",
    "# For data processing and analysis\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üìä Legislative Bill Mentions Data Collector\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c776d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIGURATION\n",
    "START_DATE = \"2014-01-01\"  # 10 years of data\n",
    "END_DATE = \"2024-12-31\"\n",
    "OUTPUT_DIR = \"legislative_data\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Common bill patterns for identification\n",
    "BILL_PATTERNS = [\n",
    "    r'\\b[HS]\\.?\\s*(?:R\\.?\\s*|J\\.?\\s*Res\\.?\\s*|Con\\.?\\s*Res\\.?\\s*)?(\\d+)\\b',  # H.R. 1234, S. 567\n",
    "    r'\\b(?:House|Senate)\\s+(?:Bill|Resolution)\\s+(\\d+)\\b',  # House Bill 1234\n",
    "    r'\\b(?:HR|SR)\\s*-?\\s*(\\d+)\\b',  # HR-1234, SR 567\n",
    "]\n",
    "\n",
    "def extract_bill_numbers(text: str) -> List[str]:\n",
    "    \"\"\"Extract bill numbers from text using regex patterns\"\"\"\n",
    "    bills = []\n",
    "    for pattern in BILL_PATTERNS:\n",
    "        matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            bills.append(match.group(0))\n",
    "    return list(set(bills))  # Remove duplicates\n",
    "\n",
    "def save_data(data: List[Dict], filename: str):\n",
    "    \"\"\"Save data to JSON and CSV formats\"\"\"\n",
    "    # Save as JSON\n",
    "    json_path = os.path.join(OUTPUT_DIR, f\"{filename}.json\")\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save as CSV\n",
    "    if data:\n",
    "        df = pd.DataFrame(data)\n",
    "        csv_path = os.path.join(OUTPUT_DIR, f\"{filename}.csv\")\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ Saved {len(data)} records to {json_path} and {csv_path}\")\n",
    "    return len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¥ REDDIT DATA COLLECTION\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 1: REDDIT DATA COLLECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüî¥ REDDIT DATA COLLECTION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "class RedditBillScraper:\n",
    "    def __init__(self):\n",
    "        if not REDDIT_AVAILABLE:\n",
    "            raise ImportError(\"Reddit packages (praw, pmaw) not available\")\n",
    "        \n",
    "        # Using Pushshift for historical data\n",
    "        self.api = PushshiftAPI()\n",
    "        \n",
    "        # Political subreddits to search\n",
    "        self.subreddits = [\n",
    "            'politics', 'Conservative', 'NeutralPolitics', 'Ask_Politics',\n",
    "            'PoliticalDiscussion', 'moderatepolitics', 'AmericanPolitics',\n",
    "            'congress', 'law', 'Legal'\n",
    "        ]\n",
    "    \n",
    "    def search_submissions(self, query: str, subreddit: str, start_date: str, end_date: str) -> List[Dict]:\n",
    "        \"\"\"Search Reddit submissions for bill mentions\"\"\"\n",
    "        print(f\"  Searching r/{subreddit} for: {query}\")\n",
    "        \n",
    "        try:\n",
    "            submissions = self.api.search_submissions(\n",
    "                q=query,\n",
    "                subreddit=subreddit,\n",
    "                after=start_date,\n",
    "                before=end_date,\n",
    "                limit=1000\n",
    "            )\n",
    "            \n",
    "            results = []\n",
    "            for submission in submissions:\n",
    "                if hasattr(submission, 'title') and hasattr(submission, 'selftext'):\n",
    "                    # Extract bill mentions from title and text\n",
    "                    title_bills = extract_bill_numbers(submission.title)\n",
    "                    text_bills = extract_bill_numbers(submission.selftext or \"\")\n",
    "                    \n",
    "                    if title_bills or text_bills:\n",
    "                        results.append({\n",
    "                            'id': submission.id,\n",
    "                            'title': submission.title,\n",
    "                            'text': submission.selftext,\n",
    "                            'author': str(submission.author),\n",
    "                            'subreddit': submission.subreddit,\n",
    "                            'created_utc': submission.created_utc,\n",
    "                            'score': submission.score,\n",
    "                            'num_comments': submission.num_comments,\n",
    "                            'url': f\"https://reddit.com{submission.permalink}\",\n",
    "                            'bills_mentioned': list(set(title_bills + text_bills)),\n",
    "                            'source': 'reddit'\n",
    "                        })\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Error searching r/{subreddit}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def collect_bill_mentions(self) -> List[Dict]:\n",
    "        \"\"\"Collect bill mentions from multiple subreddits\"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        # Search terms related to legislation\n",
    "        search_terms = [\n",
    "            \"H.R.\", \"S.\", \"House Bill\", \"Senate Bill\", \"legislation\",\n",
    "            \"Congress\", \"bill passed\", \"vote\", \"amendment\"\n",
    "        ]\n",
    "        \n",
    "        for term in search_terms:\n",
    "            for subreddit in self.subreddits:\n",
    "                try:\n",
    "                    results = self.search_submissions(term, subreddit, START_DATE, END_DATE)\n",
    "                    all_results.extend(results)\n",
    "                    time.sleep(1)  # Rate limiting\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è  Error with {subreddit}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        # Remove duplicates based on post ID\n",
    "        seen_ids = set()\n",
    "        unique_results = []\n",
    "        for result in all_results:\n",
    "            if result['id'] not in seen_ids:\n",
    "                seen_ids.add(result['id'])\n",
    "                unique_results.append(result)\n",
    "        \n",
    "        return unique_results\n",
    "\n",
    "class ImprovedRedditScraper:\n",
    "    \"\"\"Improved Reddit scraper with better error handling and diagnostics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        self.subreddits = ['politics', 'Conservative', 'NeutralPolitics', 'congress']\n",
    "        self.base_delay = 2\n",
    "    \n",
    "    def search_reddit_json(self, subreddit: str, query: str, time_filter: str = 'all', limit: int = 100) -> List[Dict]:\n",
    "        \"\"\"Search Reddit using public JSON API\"\"\"\n",
    "        url = f\"https://www.reddit.com/r/{subreddit}/search.json\"\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'restrict_sr': 'on',\n",
    "            'sort': 'relevance',\n",
    "            't': time_filter,\n",
    "            'limit': limit\n",
    "        }\n",
    "        \n",
    "        print(f\"  Searching r/{subreddit} for '{query}'...\", end=' ')\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, params=params, timeout=15)\n",
    "            \n",
    "            if response.status_code == 429:\n",
    "                print(\"‚ö†Ô∏è  Rate limited, waiting...\")\n",
    "                time.sleep(60)\n",
    "                return []\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            posts = data.get('data', {}).get('children', [])\n",
    "            results = []\n",
    "            \n",
    "            for post in posts:\n",
    "                post_data = post.get('data', {})\n",
    "                \n",
    "                title = post_data.get('title', '')\n",
    "                selftext = post_data.get('selftext', '')\n",
    "                \n",
    "                # Extract bill mentions\n",
    "                title_bills = extract_bill_numbers(title)\n",
    "                text_bills = extract_bill_numbers(selftext)\n",
    "                \n",
    "                # More flexible matching\n",
    "                combined_text = (title + ' ' + selftext).lower()\n",
    "                has_legislative = any(term in combined_text for term in \n",
    "                                    ['h.r.', 's.', 'h.r ', 's. ', 'hr ', 'senate bill', \n",
    "                                     'house bill', 'congress', 'legislation', 'congressional'])\n",
    "                \n",
    "                if title_bills or text_bills or has_legislative:\n",
    "                    results.append({\n",
    "                        'id': post_data.get('id', ''),\n",
    "                        'title': title,\n",
    "                        'text': selftext,\n",
    "                        'author': post_data.get('author', ''),\n",
    "                        'subreddit': subreddit,\n",
    "                        'created_utc': post_data.get('created_utc', 0),\n",
    "                        'score': post_data.get('score', 0),\n",
    "                        'num_comments': post_data.get('num_comments', 0),\n",
    "                        'url': f\"https://reddit.com{post_data.get('permalink', '')}\",\n",
    "                        'bills_mentioned': list(set(title_bills + text_bills)),\n",
    "                        'source': 'reddit_improved'\n",
    "                    })\n",
    "            \n",
    "            print(f\"‚úì Found {len(results)} relevant posts\")\n",
    "            return results\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚úó Error: {str(e)}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Unexpected error: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def get_subreddit_hot(self, subreddit: str, limit: int = 100) -> List[Dict]:\n",
    "        \"\"\"Get hot posts from subreddit (no search query needed)\"\"\"\n",
    "        url = f\"https://www.reddit.com/r/{subreddit}/hot.json\"\n",
    "        params = {'limit': limit}\n",
    "        \n",
    "        print(f\"  Getting hot posts from r/{subreddit}...\", end=' ')\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, params=params, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            posts = data.get('data', {}).get('children', [])\n",
    "            results = []\n",
    "            \n",
    "            for post in posts:\n",
    "                post_data = post.get('data', {})\n",
    "                title = post_data.get('title', '')\n",
    "                selftext = post_data.get('selftext', '')\n",
    "                \n",
    "                # Check for legislative content\n",
    "                combined = (title + ' ' + selftext).lower()\n",
    "                if any(term in combined for term in ['h.r.', 's.', 'bill', 'congress', 'senate', 'house']):\n",
    "                    title_bills = extract_bill_numbers(title)\n",
    "                    text_bills = extract_bill_numbers(selftext)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'id': post_data.get('id', ''),\n",
    "                        'title': title,\n",
    "                        'text': selftext,\n",
    "                        'author': post_data.get('author', ''),\n",
    "                        'subreddit': subreddit,\n",
    "                        'created_utc': post_data.get('created_utc', 0),\n",
    "                        'score': post_data.get('score', 0),\n",
    "                        'num_comments': post_data.get('num_comments', 0),\n",
    "                        'url': f\"https://reddit.com{post_data.get('permalink', '')}\",\n",
    "                        'bills_mentioned': list(set(title_bills + text_bills)),\n",
    "                        'source': 'reddit_hot'\n",
    "                    })\n",
    "            \n",
    "            print(f\"‚úì Found {len(results)} legislative posts\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def collect_bill_mentions(self) -> List[Dict]:\n",
    "        \"\"\"Collect bill mentions using multiple strategies\"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        # Strategy 1: Search specific terms\n",
    "        search_terms = [\n",
    "            \"H.R.\",\n",
    "            \"Senate bill\", \n",
    "            \"House bill\",\n",
    "            \"Congress passed\",\n",
    "            \"legislation\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nüìç Strategy 1: Searching with specific terms\")\n",
    "        for term in search_terms:\n",
    "            for subreddit in self.subreddits:\n",
    "                try:\n",
    "                    results = self.search_reddit_json(subreddit, term, time_filter='year', limit=100)\n",
    "                    all_results.extend(results)\n",
    "                    time.sleep(self.base_delay)\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è  Error with r/{subreddit}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        # Strategy 2: Get hot posts and filter\n",
    "        print(\"\\nüìç Strategy 2: Filtering hot/recent posts\")\n",
    "        for subreddit in self.subreddits:\n",
    "            try:\n",
    "                results = self.get_subreddit_hot(subreddit, limit=100)\n",
    "                all_results.extend(results)\n",
    "                time.sleep(self.base_delay)\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Error with r/{subreddit}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen_ids = set()\n",
    "        unique_results = []\n",
    "        for result in all_results:\n",
    "            if result['id'] not in seen_ids:\n",
    "                seen_ids.add(result['id'])\n",
    "                unique_results.append(result)\n",
    "        \n",
    "        print(f\"\\nüìä Total unique Reddit posts found: {len(unique_results)}\")\n",
    "        return unique_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ab0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¥ REDDIT DATA COLLECTION\n",
      "------------------------------\n",
      "\n",
      "üìç Strategy 1: Searching with specific terms\n",
      "  Searching r/politics for 'H.R.'... ‚úì Found 99 relevant posts\n",
      "  Searching r/Conservative for 'H.R.'... ‚úì Found 9 relevant posts\n",
      "  Searching r/NeutralPolitics for 'H.R.'... ‚úì Found 1 relevant posts\n",
      "  Searching r/congress for 'H.R.'... ‚úì Found 13 relevant posts\n",
      "  Searching r/politics for 'Senate bill'... ‚úì Found 19 relevant posts\n",
      "  Searching r/Conservative for 'Senate bill'... ‚úì Found 13 relevant posts\n",
      "  Searching r/NeutralPolitics for 'Senate bill'... ‚úì Found 9 relevant posts\n",
      "  Searching r/congress for 'Senate bill'... ‚úì Found 20 relevant posts\n",
      "  Searching r/politics for 'House bill'... ‚úì Found 13 relevant posts\n",
      "  Searching r/Conservative for 'House bill'... ‚úì Found 15 relevant posts\n",
      "  Searching r/NeutralPolitics for 'House bill'... ‚úì Found 7 relevant posts\n",
      "  Searching r/congress for 'House bill'... ‚úì Found 23 relevant posts\n",
      "  Searching r/politics for 'Congress passed'... ‚úì Found 35 relevant posts\n",
      "  Searching r/Conservative for 'Congress passed'... ‚úì Found 19 relevant posts\n",
      "  Searching r/NeutralPolitics for 'Congress passed'... ‚úì Found 4 relevant posts\n",
      "  Searching r/congress for 'Congress passed'... ‚úì Found 14 relevant posts\n",
      "  Searching r/politics for 'legislation'... ‚úì Found 69 relevant posts\n",
      "  Searching r/Conservative for 'legislation'... ‚úì Found 49 relevant posts\n",
      "  Searching r/NeutralPolitics for 'legislation'... ‚úì Found 5 relevant posts\n",
      "  Searching r/congress for 'legislation'... ‚úì Found 39 relevant posts\n",
      "\n",
      "üìç Strategy 2: Filtering hot/recent posts\n",
      "  Getting hot posts from r/politics... ‚úì Found 22 legislative posts\n",
      "  Getting hot posts from r/Conservative... ‚úì Found 15 legislative posts\n",
      "  Getting hot posts from r/NeutralPolitics... ‚úì Found 84 legislative posts\n",
      "  Getting hot posts from r/congress... ‚úì Found 62 legislative posts\n",
      "\n",
      "üìä Total unique Reddit posts found: 562\n",
      "‚úÖ Saved 562 records to legislative_data/reddit_bill_mentions.json and legislative_data/reddit_bill_mentions.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize Reddit scraper and collect data\n",
    "reddit_data = []\n",
    "reddit_count = 0\n",
    "\n",
    "print(\"üî¥ REDDIT DATA COLLECTION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    improved_scraper = ImprovedRedditScraper()\n",
    "    reddit_data = improved_scraper.collect_bill_mentions()\n",
    "    \n",
    "    if reddit_data:\n",
    "        reddit_count = save_data(reddit_data, \"reddit_bill_mentions\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No Reddit data collected. This could be due to:\")\n",
    "        print(\"  - Rate limiting (try again in a few minutes)\")\n",
    "        print(\"  - Search terms not matching recent content\")\n",
    "        print(\"  - Reddit API issues\")\n",
    "        print(\"\\nüí° Tip: You can still proceed with Guardian data (26k articles is excellent!)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error collecting Reddit data: {e}\")\n",
    "    print(\"Continuing with Guardian data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dafbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì∞ GUARDIAN API DATA COLLECTION\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 2: GUARDIAN API DATA COLLECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüì∞ GUARDIAN API DATA COLLECTION\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "class GuardianScraper:\n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        # Guardian API is free - get key at: https://open-platform.theguardian.com/access/\n",
    "        self.api_key = api_key or \"test\"\n",
    "        self.base_url = \"https://content.guardianapis.com/search\"\n",
    "    \n",
    "    def search_articles(self, query: str, from_date: str, to_date: str, page_size: int = 50) -> List[Dict]:\n",
    "        \"\"\"Search Guardian articles for legislative content\"\"\"\n",
    "        print(f\"  Searching Guardian for: {query}\")\n",
    "        \n",
    "        all_articles = []\n",
    "        page = 1\n",
    "        \n",
    "        while True:\n",
    "            params = {\n",
    "                'q': query,\n",
    "                'from-date': from_date,\n",
    "                'to-date': to_date,\n",
    "                'page-size': page_size,\n",
    "                'page': page,\n",
    "                'show-fields': 'headline,bodyText,byline,firstPublicationDate',\n",
    "                'section': 'us-news|world/usa',\n",
    "                'api-key': self.api_key\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(self.base_url, params=params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                if data['response']['status'] != 'ok':\n",
    "                    break\n",
    "                \n",
    "                articles = data['response']['results']\n",
    "                if not articles:\n",
    "                    break\n",
    "                \n",
    "                for article in articles:\n",
    "                    # Extract bill mentions from headline and body\n",
    "                    headline = article.get('fields', {}).get('headline', '')\n",
    "                    body = article.get('fields', {}).get('bodyText', '')\n",
    "                    \n",
    "                    headline_bills = extract_bill_numbers(headline)\n",
    "                    body_bills = extract_bill_numbers(body)\n",
    "                    \n",
    "                    if headline_bills or body_bills or any(term in (headline + body).lower() \n",
    "                                                         for term in ['congress', 'senate', 'house', 'bill', 'legislation']):\n",
    "                        all_articles.append({\n",
    "                            'id': article['id'],\n",
    "                            'headline': headline,\n",
    "                            'body': body,\n",
    "                            'byline': article.get('fields', {}).get('byline', ''),\n",
    "                            'publication_date': article.get('fields', {}).get('firstPublicationDate', ''),\n",
    "                            'web_url': article['webUrl'],\n",
    "                            'section': article.get('sectionName', ''),\n",
    "                            'bills_mentioned': list(set(headline_bills + body_bills)),\n",
    "                            'source': 'guardian'\n",
    "                        })\n",
    "                \n",
    "                page += 1\n",
    "                time.sleep(0.1)  # Rate limiting\n",
    "                \n",
    "                if page > data['response']['pages']:\n",
    "                    break\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Error fetching page {page}: {str(e)}\")\n",
    "                break\n",
    "        \n",
    "        return all_articles\n",
    "    \n",
    "    def collect_legislative_coverage(self) -> List[Dict]:\n",
    "        \"\"\"Collect legislative coverage from Guardian\"\"\"\n",
    "        all_articles = []\n",
    "        \n",
    "        # Search terms for legislative content\n",
    "        search_queries = [\n",
    "            'Congress bill', 'Senate vote', 'House Representatives',\n",
    "            'legislation passed', 'congressional vote', 'federal law',\n",
    "            'H.R.', 'Senate bill', 'House bill'\n",
    "        ]\n",
    "        \n",
    "        for query in search_queries:\n",
    "            try:\n",
    "                articles = self.search_articles(query, START_DATE, END_DATE)\n",
    "                all_articles.extend(articles)\n",
    "                time.sleep(1)  # Rate limiting between queries\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Error with query '{query}': {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Remove duplicates based on article ID\n",
    "        seen_ids = set()\n",
    "        unique_articles = []\n",
    "        for article in all_articles:\n",
    "            if article['id'] not in seen_ids:\n",
    "                seen_ids.add(article['id'])\n",
    "                unique_articles.append(article)\n",
    "        \n",
    "        return unique_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e84aa8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Guardian data...\n",
      "  Searching Guardian for: Congress bill\n",
      "  Searching Guardian for: Senate vote\n",
      "  Searching Guardian for: House Representatives\n",
      "  Searching Guardian for: legislation passed\n",
      "  Searching Guardian for: congressional vote\n",
      "  Searching Guardian for: federal law\n",
      "  ‚ö†Ô∏è  Error fetching page 128: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n",
      "  Searching Guardian for: H.R.\n",
      "  Searching Guardian for: Senate bill\n",
      "  Searching Guardian for: House bill\n",
      "‚úÖ Saved 25097 records to legislative_data/guardian_bill_mentions.json and legislative_data/guardian_bill_mentions.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize Guardian scraper\n",
    "guardian_scraper = GuardianScraper()\n",
    "\n",
    "# Collect Guardian data\n",
    "print(\"Collecting Guardian data...\")\n",
    "guardian_data = guardian_scraper.collect_legislative_coverage()\n",
    "guardian_count = save_data(guardian_data, \"guardian_bill_mentions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2bf159e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê COMMON CRAWL DATA COLLECTION\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 3: COMMON CRAWL DATA COLLECTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüåê COMMON CRAWL DATA COLLECTION\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "class CommonCrawlScraper:\n",
    "    def __init__(self):\n",
    "        self.index_url = \"https://index.commoncrawl.org/CC-MAIN-{}-index\"\n",
    "        self.available_indexes = self._get_available_indexes()\n",
    "    \n",
    "    def _get_available_indexes(self) -> List[str]:\n",
    "        \"\"\"Get available Common Crawl indexes for the date range\"\"\"\n",
    "        # Common Crawl releases monthly archives\n",
    "        # Format: CC-MAIN-YYYY-WW (where WW is week number)\n",
    "        recent_indexes = [\n",
    "            \"2024-51\", \"2024-46\", \"2024-42\", \"2024-38\", \"2024-33\",\n",
    "            \"2023-50\", \"2023-40\", \"2023-23\", \"2023-14\", \"2023-06\",\n",
    "            \"2022-49\", \"2022-40\", \"2022-33\", \"2022-27\", \"2022-21\",\n",
    "            \"2021-49\", \"2021-43\", \"2021-39\", \"2021-31\", \"2021-25\"\n",
    "        ]\n",
    "        \n",
    "        return recent_indexes\n",
    "    \n",
    "    def search_crawl_index(self, domain: str, index: str, max_results: int = 50) -> List[Dict]:\n",
    "        \"\"\"Search Common Crawl INDEX only (metadata, not full content)\"\"\"\n",
    "        print(f\"  Searching {domain} in index {index}\")\n",
    "        \n",
    "        url = self.index_url.format(index)\n",
    "        params = {\n",
    "            'url': f\"{domain}/*\",\n",
    "            'output': 'json',\n",
    "            'limit': max_results,\n",
    "            'filter': '=status:200'  # Only successful pages\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            results = []\n",
    "            for line in response.text.strip().split('\\n'):\n",
    "                if line:\n",
    "                    try:\n",
    "                        data = json.loads(line)\n",
    "                        # Filter for legislative content in URL\n",
    "                        url_lower = data.get('url', '').lower()\n",
    "                        if any(term in url_lower for term in ['congress', 'bill', 'senate', 'house', 'legislation', 'law', 'vote']):\n",
    "                            results.append(data)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "            \n",
    "            print(f\"    Found {len(results)} legislative URLs\")\n",
    "            return results\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Error searching {domain} in {index}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def collect_metadata_only(self) -> List[Dict]:\n",
    "        \"\"\"Collect URL metadata only (faster, no content fetching)\"\"\"\n",
    "        # Target news domains known for political coverage\n",
    "        news_domains = [\n",
    "            'politico.com', 'thehill.com', 'rollcall.com', \n",
    "            'congress.gov', 'npr.org'\n",
    "        ]\n",
    "        \n",
    "        all_mentions = []\n",
    "        \n",
    "        for domain in news_domains:\n",
    "            for index in self.available_indexes[:3]:  # Limit to 3 most recent\n",
    "                try:\n",
    "                    results = self.search_crawl_index(domain, index, max_results=30)\n",
    "                    \n",
    "                    for result in results:\n",
    "                        url = result.get('url', '')\n",
    "                        timestamp = result.get('timestamp', '')\n",
    "                        \n",
    "                        # Extract bill mentions from URL itself\n",
    "                        bills_in_url = extract_bill_numbers(url)\n",
    "                        \n",
    "                        all_mentions.append({\n",
    "                            'url': url,\n",
    "                            'domain': domain,\n",
    "                            'timestamp': timestamp,\n",
    "                            'bills_mentioned': bills_in_url,\n",
    "                            'crawl_index': index,\n",
    "                            'mime_type': result.get('mime', ''),\n",
    "                            'source': 'common_crawl_metadata'\n",
    "                        })\n",
    "                    \n",
    "                    time.sleep(1)  # Rate limiting\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è  Error processing {domain} in {index}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        return all_mentions\n",
    "\n",
    "\n",
    "class AlternativeNewsScraper:\n",
    "    \"\"\"Alternative scraper using direct RSS feeds and sitemaps\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (compatible; AcademicResearch/1.0)'\n",
    "        }\n",
    "    \n",
    "    def scrape_politico_rss(self) -> List[Dict]:\n",
    "        \"\"\"Scrape Politico RSS feed\"\"\"\n",
    "        print(\"  Fetching Politico RSS feed...\")\n",
    "        \n",
    "        rss_urls = [\n",
    "            'https://www.politico.com/rss/congress.xml',\n",
    "            'https://www.politico.com/rss/politics08.xml'\n",
    "        ]\n",
    "        \n",
    "        all_articles = []\n",
    "        \n",
    "        for rss_url in rss_urls:\n",
    "            try:\n",
    "                response = requests.get(rss_url, headers=self.headers, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Simple XML parsing without external libraries\n",
    "                content = response.text\n",
    "                \n",
    "                # Extract titles and links (basic regex parsing)\n",
    "                import re\n",
    "                items = re.findall(r'<item>(.*?)</item>', content, re.DOTALL)\n",
    "                \n",
    "                for item in items:\n",
    "                    title_match = re.search(r'<title>(.*?)</title>', item)\n",
    "                    link_match = re.search(r'<link>(.*?)</link>', item)\n",
    "                    pub_date_match = re.search(r'<pubDate>(.*?)</pubDate>', item)\n",
    "                    \n",
    "                    if title_match and link_match:\n",
    "                        title = title_match.group(1)\n",
    "                        link = link_match.group(1)\n",
    "                        pub_date = pub_date_match.group(1) if pub_date_match else ''\n",
    "                        \n",
    "                        # Extract bill mentions\n",
    "                        bills = extract_bill_numbers(title)\n",
    "                        \n",
    "                        if bills or any(term in title.lower() for term in ['congress', 'senate', 'house', 'bill']):\n",
    "                            all_articles.append({\n",
    "                                'title': title,\n",
    "                                'url': link,\n",
    "                                'publication_date': pub_date,\n",
    "                                'bills_mentioned': bills,\n",
    "                                'source': 'politico_rss'\n",
    "                            })\n",
    "                \n",
    "                print(f\"    Found {len(all_articles)} articles\")\n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Error fetching {rss_url}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return all_articles\n",
    "    \n",
    "    def scrape_thehill_recent(self) -> List[Dict]:\n",
    "        \"\"\"Scrape The Hill recent articles\"\"\"\n",
    "        print(\"  Fetching The Hill recent articles...\")\n",
    "        \n",
    "        try:\n",
    "            url = \"https://thehill.com/policy/national-security/\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            content = response.text\n",
    "            articles = []\n",
    "            \n",
    "            # Extract article titles and links\n",
    "            import re\n",
    "            article_pattern = r'<a[^>]*href=\"([^\"]*)\"[^>]*>([^<]*(?:H\\.R\\.|S\\.|bill|Congress|Senate|House)[^<]*)</a>'\n",
    "            matches = re.findall(article_pattern, content, re.IGNORECASE)\n",
    "            \n",
    "            for link, title in matches:\n",
    "                bills = extract_bill_numbers(title)\n",
    "                \n",
    "                if bills or any(term in title.lower() for term in ['congress', 'bill', 'legislation']):\n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'url': link if link.startswith('http') else f\"https://thehill.com{link}\",\n",
    "                        'bills_mentioned': bills,\n",
    "                        'source': 'thehill_scrape'\n",
    "                    })\n",
    "            \n",
    "            print(f\"    Found {len(articles)} articles\")\n",
    "            return articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Error fetching The Hill: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def collect_alternative_sources(self) -> List[Dict]:\n",
    "        \"\"\"Collect from alternative sources\"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        # Politico RSS\n",
    "        all_data.extend(self.scrape_politico_rss())\n",
    "        \n",
    "        # The Hill\n",
    "        all_data.extend(self.scrape_thehill_recent())\n",
    "        \n",
    "        return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Note: Common Crawl with full content fetching often has connection issues.\n",
      "Using metadata-only approach + alternative RSS/direct scraping.\n",
      "\n",
      "Collecting Common Crawl metadata...\n",
      "  Searching politico.com in index 2024-51\n",
      "    Found 0 legislative URLs\n",
      "  Searching politico.com in index 2024-46\n",
      "    Found 0 legislative URLs\n",
      "  Searching politico.com in index 2024-42\n",
      "    Found 0 legislative URLs\n",
      "  Searching thehill.com in index 2024-51\n",
      "  ‚ö†Ô∏è  Error searching thehill.com in 2024-51: 404 Client Error: Not Found for url: https://index.commoncrawl.org/CC-MAIN-2024-51-index?url=thehill.com%2F%2A&output=json&limit=30&filter=%3Dstatus%3A200\n",
      "  Searching thehill.com in index 2024-46\n",
      "  ‚ö†Ô∏è  Error searching thehill.com in 2024-46: 404 Client Error: Not Found for url: https://index.commoncrawl.org/CC-MAIN-2024-46-index?url=thehill.com%2F%2A&output=json&limit=30&filter=%3Dstatus%3A200\n",
      "  Searching thehill.com in index 2024-42\n",
      "  ‚ö†Ô∏è  Error searching thehill.com in 2024-42: 404 Client Error: Not Found for url: https://index.commoncrawl.org/CC-MAIN-2024-42-index?url=thehill.com%2F%2A&output=json&limit=30&filter=%3Dstatus%3A200\n",
      "  Searching rollcall.com in index 2024-51\n",
      "    Found 4 legislative URLs\n",
      "  Searching rollcall.com in index 2024-46\n",
      "    Found 7 legislative URLs\n",
      "  Searching rollcall.com in index 2024-42\n",
      "    Found 4 legislative URLs\n",
      "  Searching congress.gov in index 2024-51\n",
      "  ‚ö†Ô∏è  Error searching congress.gov in 2024-51: 404 Client Error: Not Found for url: https://index.commoncrawl.org/CC-MAIN-2024-51-index?url=congress.gov%2F%2A&output=json&limit=30&filter=%3Dstatus%3A200\n",
      "  Searching congress.gov in index 2024-46\n",
      "  ‚ö†Ô∏è  Error searching congress.gov in 2024-46: 404 Client Error: Not Found for url: https://index.commoncrawl.org/CC-MAIN-2024-46-index?url=congress.gov%2F%2A&output=json&limit=30&filter=%3Dstatus%3A200\n",
      "  Searching congress.gov in index 2024-42\n",
      "  ‚ö†Ô∏è  Error searching congress.gov in 2024-42: 404 Client Error: Not Found for url: https://index.commoncrawl.org/CC-MAIN-2024-42-index?url=congress.gov%2F%2A&output=json&limit=30&filter=%3Dstatus%3A200\n",
      "  Searching npr.org in index 2024-51\n",
      "    Found 0 legislative URLs\n",
      "  Searching npr.org in index 2024-46\n",
      "    Found 0 legislative URLs\n",
      "  Searching npr.org in index 2024-42\n",
      "    Found 0 legislative URLs\n",
      "\n",
      "Collecting from alternative news sources (RSS feeds)...\n",
      "  Fetching Politico RSS feed...\n",
      "  ‚ö†Ô∏è  Error fetching https://www.politico.com/rss/congress.xml: 403 Client Error: Forbidden for url: https://www.politico.com/rss/congress.xml\n",
      "  ‚ö†Ô∏è  Error fetching https://www.politico.com/rss/politics08.xml: 403 Client Error: Forbidden for url: https://www.politico.com/rss/politics08.xml\n",
      "  Fetching The Hill recent articles...\n",
      "  ‚ö†Ô∏è  Error fetching The Hill: 403 Client Error: Forbidden for url: https://thehill.com/policy/national-security/\n",
      "‚úÖ Saved 15 records to legislative_data/news_sources_bill_mentions.json and legislative_data/news_sources_bill_mentions.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize Common Crawl scraper\n",
    "print(\"‚ö†Ô∏è  Note: Common Crawl with full content fetching often has connection issues.\")\n",
    "print(\"Using metadata-only approach + alternative RSS/direct scraping.\")\n",
    "\n",
    "crawl_data = []\n",
    "crawl_count = 0\n",
    "\n",
    "try:\n",
    "    print(\"\\nCollecting Common Crawl metadata...\")\n",
    "    crawl_scraper = CommonCrawlScraper()\n",
    "    crawl_data = crawl_scraper.collect_metadata_only()\n",
    "    \n",
    "    print(\"\\nCollecting from alternative news sources (RSS feeds)...\")\n",
    "    alt_scraper = AlternativeNewsScraper()\n",
    "    alt_data = alt_scraper.collect_alternative_sources()\n",
    "    crawl_data.extend(alt_data)\n",
    "    \n",
    "    crawl_count = save_data(crawl_data, \"news_sources_bill_mentions\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error with news source collection: {e}\")\n",
    "    print(\"Continuing with Reddit and Guardian data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd58c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä DATA COLLECTION SUMMARY\n",
      "========================================\n",
      "Reddit mentions: 562\n",
      "Guardian articles: 25097\n",
      "Common Crawl mentions: 15\n",
      "Total records: 25674\n",
      "\n",
      "üìÅ Data saved in 'legislative_data' directory\n",
      "\n",
      "üîç PRELIMINARY ANALYSIS\n",
      "-------------------------\n",
      "Records by source:\n",
      "source\n",
      "guardian                 25097\n",
      "reddit_improved            395\n",
      "reddit_hot                 167\n",
      "common_crawl_metadata       15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Most mentioned bills:\n",
      "  s 2020: 445 mentions\n",
      "  s 2016: 435 mentions\n",
      "  s 2024: 270 mentions\n",
      "  s 2008: 111 mentions\n",
      "  s 10: 104 mentions\n",
      "  s 2012: 99 mentions\n",
      "  s 100: 92 mentions\n",
      "  s 2018: 84 mentions\n",
      "  s 7: 82 mentions\n",
      "  s 2: 79 mentions\n",
      "\n",
      "‚úÖ Combined dataset saved to legislative_data/combined_bill_mentions.csv\n",
      "\n",
      "üéØ NEXT STEPS:\n",
      "1. Clean and deduplicate the collected data\n",
      "2. Enhance bill number extraction with more sophisticated NLP\n",
      "3. Match mentions to actual bills using Congress.gov API\n",
      "4. Add sentiment analysis to the mentions\n",
      "5. Temporal analysis of mention patterns\n",
      "6. Integrate with voting records and bill outcomes\n",
      "\n",
      "üöÄ Happy researching! Your Erdos Institute project data collection is complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA ANALYSIS AND SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüìä DATA COLLECTION SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Reddit mentions: {reddit_count}\")\n",
    "print(f\"Guardian articles: {guardian_count}\")\n",
    "print(f\"Common Crawl mentions: {crawl_count}\")\n",
    "print(f\"Total records: {reddit_count + guardian_count + crawl_count}\")\n",
    "print(f\"\\nüìÅ Data saved in '{OUTPUT_DIR}' directory\")\n",
    "\n",
    "# Combine all data for analysis\n",
    "all_data = reddit_data + guardian_data + crawl_data\n",
    "\n",
    "if all_data:\n",
    "    # Create summary DataFrame\n",
    "    df_all = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Basic analysis\n",
    "    print(f\"\\nüîç PRELIMINARY ANALYSIS\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    # Source distribution\n",
    "    source_counts = df_all['source'].value_counts()\n",
    "    print(f\"Records by source:\\n{source_counts}\")\n",
    "    \n",
    "    # Bill mention frequency\n",
    "    all_bills = []\n",
    "    for record in all_data:\n",
    "        all_bills.extend(record.get('bills_mentioned', []))\n",
    "    \n",
    "    if all_bills:\n",
    "        bill_counts = Counter(all_bills)\n",
    "        print(f\"\\nMost mentioned bills:\")\n",
    "        for bill, count in bill_counts.most_common(10):\n",
    "            print(f\"  {bill}: {count} mentions\")\n",
    "    \n",
    "    # Save combined dataset\n",
    "    combined_path = os.path.join(OUTPUT_DIR, \"combined_bill_mentions.csv\")\n",
    "    df_all.to_csv(combined_path, index=False, encoding='utf-8')\n",
    "    print(f\"\\n‚úÖ Combined dataset saved to {combined_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
